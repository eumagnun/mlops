{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "Dados_Sint√©ticos_para_Teste_do_Google.ipynb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWh7LUPcSuM-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# --- PASSO 0: DEFINI√á√ïES INICIAIS E PAR√ÇMETROS ---\n",
        "# Par√¢metros fornecidos para guiar a gera√ß√£o de dados\n",
        "FREQ_PARCIAL_ALVO = 0.03   # 3%\n",
        "FREQ_TOTAL_ALVO = 0.001  # 0.1%\n",
        "SEV_PARCIAL_ALVO = 3_000 # R$ 3.000\n",
        "\n",
        "# Par√¢metros para a simula√ß√£o\n",
        "N_APOLICES = 1000000\n",
        "np.random.seed(42) # Para reprodutibilidade dos resultados"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PASSO 1: CRIA√á√ÉO DO DATASET SINT√âTICO ---\n",
        "print(f\"Gerando um dataset sint√©tico com {N_APOLICES} ap√≥lices...\")\n",
        "\n",
        "# 1.1. Cria√ß√£o das features preditivas (caracter√≠sticas do risco)\n",
        "df = pd.DataFrame({\n",
        "    'exposicao': np.random.uniform(0.1, 1.0, N_APOLICES), # Exposi√ß√£o entre 1.2 meses e 1 ano\n",
        "    'classe_bonus': np.random.randint(0, 11, N_APOLICES),\n",
        "    'idade': np.random.randint(18, 81, N_APOLICES),\n",
        "    'rns': np.random.randint(0, 2, N_APOLICES), # 0 = sem RNS, 1 = com RNS\n",
        "    'valor_veiculo': np.random.uniform(20000, 150001, N_APOLICES).round(2)\n",
        "})\n",
        "\n",
        "print(f\"Gerando um dataset sint√©tico com dimens√µes: {df.shape}\")\n",
        "\n",
        "# 1.2. Cria√ß√£o do \"fator de risco\" individual para cada ap√≥lice\n",
        "# Usamos uma combina√ß√£o linear das features para simular o risco latente.\n",
        "# Sinais dos coeficientes:\n",
        "# - classe_bonus: negativo (maior b√¥nus, menor risco)\n",
        "# - idade: negativo (maior idade, menor risco)\n",
        "# - rns: positivo (ter RNS, maior risco)\n",
        "log_risco_base_freq = (\n",
        "    -0.15 * df['classe_bonus']\n",
        "    -0.02 * df['idade']\n",
        "    + 1.0 * df['rns']\n",
        ")\n",
        "\n",
        "# 1.3. Gera√ß√£o das Quantidades de Sinistros (Frequ√™ncia)\n",
        "# Ajustamos o intercepto para que a m√©dia da frequ√™ncia na carteira se aproxime dos alvos.\n",
        "\n",
        "# Colis√£o Parcial\n",
        "# Calibra√ß√£o do intercepto para atingir a frequ√™ncia alvo\n",
        "taxa_media_parcial = np.exp(log_risco_base_freq).mean()\n",
        "intercepto_parcial = np.log(FREQ_PARCIAL_ALVO / taxa_media_parcial)\n",
        "taxa_poisson_parcial = np.exp(intercepto_parcial + log_risco_base_freq) * df['exposicao']\n",
        "df['qtd_colisao_parcial'] = np.random.poisson(taxa_poisson_parcial)\n",
        "\n",
        "# Colis√£o Total\n",
        "# Calibra√ß√£o do intercepto\n",
        "taxa_media_total = np.exp(log_risco_base_freq).mean()\n",
        "intercepto_total = np.log(FREQ_TOTAL_ALVO / taxa_media_total)\n",
        "taxa_poisson_total = np.exp(intercepto_total + log_risco_base_freq) * df['exposicao']\n",
        "df['qtd_colisao_total'] = np.random.poisson(taxa_poisson_total)\n",
        "\n",
        "\n",
        "# 1.4. Gera√ß√£o da Severidade de Colis√£o Parcial (para quem teve sinistro)\n",
        "# A severidade depende do risco e, principalmente, do valor do ve√≠culo.\n",
        "log_risco_base_sev = (\n",
        "    -0.01 * df['classe_bonus']  # Influ√™ncia menor\n",
        "    -0.005 * df['idade']       # Influ√™ncia menor\n",
        "    + 0.1 * df['rns']          # Influ√™ncia pequena\n",
        "    + 0.5 * np.log(df['valor_veiculo']) # Influ√™ncia maior e n√£o-linear\n",
        ")\n",
        "\n",
        "# Calibra√ß√£o do intercepto para atingir a severidade alvo\n",
        "media_sev_base = np.exp(log_risco_base_sev[df['qtd_colisao_parcial'] > 0]).mean()\n",
        "intercepto_sev = np.log(SEV_PARCIAL_ALVO / media_sev_base)\n",
        "\n",
        "# M√©dia da Gamma para cada ap√≥lice com sinistro\n",
        "media_gamma = np.exp(intercepto_sev + log_risco_base_sev)\n",
        "\n",
        "# Par√¢metro de forma da Gamma (controla a vari√¢ncia/dispers√£o)\n",
        "shape_gamma = 20.0\n",
        "scale_gamma = media_gamma / shape_gamma\n",
        "\n",
        "# Gerar valores da Gamma apenas para quem teve sinistro\n",
        "severidade_valores = np.random.gamma(shape=shape_gamma, scale=scale_gamma)\n",
        "\n",
        "# Atribuir a severidade ao DataFrame\n",
        "df['valor_colisao_parcial'] = 0.0\n",
        "mask_sinistro_parcial = df['qtd_colisao_parcial'] > 0\n",
        "# O valor total do sinistro √© a quantidade de eventos * a severidade m√©dia por evento\n",
        "# Para simplificar, assumimos que a severidade gerada √© o valor total para aquela ap√≥lice\n",
        "df.loc[mask_sinistro_parcial, 'valor_colisao_parcial'] = \\\n",
        "    severidade_valores[mask_sinistro_parcial].round(2) * df.loc[mask_sinistro_parcial, 'qtd_colisao_parcial']\n",
        "\n",
        "# Amostra para casos com colis√£o parcial\n",
        "df.shape"
      ],
      "metadata": {
        "id": "5fjKzwhoTIut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "HhNn_j7J7aPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#write data to BQ\n",
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. CONFIGURE SUAS VARI√ÅVEIS\n",
        "# ==============================================================================\n",
        "# Substitua pelo ID do seu projeto GCP.\n",
        "project_id = \"project-poc-purple\"\n",
        "\n",
        "# Nome do dataset no BigQuery (deve existir)\n",
        "dataset_id = \"demos\"\n",
        "\n",
        "# Nome da tabela que ser√° criada ou sobrescrita\n",
        "table_id = \"dados_apolices_v1\"\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. SEU DATAFRAME\n",
        "# ==============================================================================\n",
        "# Supondo que seu DataFrame de 1 milh√£o de linhas j√° est√°\n",
        "# carregado na vari√°vel 'df'. N√£o √© preciso cri√°-lo aqui.\n",
        "# df = pd.read_csv(...) ou similar\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. ENVIE OS DADOS PARA O BIGQUERY\n",
        "# ==============================================================================\n",
        "print(\"Iniciando o carregamento para o BigQuery...\")\n",
        "\n",
        "# Inicializa o cliente, especificando o projeto para evitar erros de quota.\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "# Configura o job de carregamento.\n",
        "# 'autodetect=True' √© perfeito para o seu caso.\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    write_disposition=\"WRITE_TRUNCATE\", # Sobrescreve a tabela. Mude para WRITE_APPEND se quiser adicionar.\n",
        "    autodetect=True,\n",
        ")\n",
        "\n",
        "# Monta a refer√™ncia completa da tabela.\n",
        "table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
        "\n",
        "# Executa o job. Esta fun√ß√£o √© eficiente para grandes DataFrames. üöÄ\n",
        "job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n",
        "\n",
        "# Aguarda a finaliza√ß√£o do job.\n",
        "job.result()\n",
        "\n",
        "print(f\"‚úÖ Sucesso! Os {len(df)} registros foram carregados na tabela {table_ref}.\")"
      ],
      "metadata": {
        "id": "8PRICPvP61mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ler dados da tabela do BQ:\n",
        "\n",
        "print(f\"Lendo dados da tabela {table_ref}...\")\n",
        "sql_query = f\"\"\"\n",
        "    SELECT *\n",
        "    FROM `{table_ref}`\n",
        "\"\"\"\n",
        "df = client.query(sql_query).to_dataframe()\n",
        "\n",
        "print(\"‚úÖ Sucesso! Dados carregados no DataFrame.\")\n",
        "print(\"-\" * 30)\n",
        "print(\"Informa√ß√µes do DataFrame carregado:\")\n",
        "df.info()\n",
        "colunas_para_converter = df.select_dtypes(include='Int64').columns\n",
        "for col in colunas_para_converter:\n",
        "    df[col] = df[col].astype('int64')\n",
        "df.info()"
      ],
      "metadata": {
        "id": "svK-q1gT9E4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q-AtSyGWfGXT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}