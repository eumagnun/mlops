{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "Dados_Sintéticos_para_Teste_do_Google_plus_pipeline.ipynb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install kfp google-cloud-pipeline-components --upgrade\n",
        "!pip install google-cloud-aiplatform\n",
        "!pip install --upgrade google-cloud-storage"
      ],
      "metadata": {
        "id": "dNp-AEI6Y010"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kfp\n",
        "from kfp import dsl\n",
        "from kfp.dsl import Input, Output, Dataset, Model, Markdown, HTML\n",
        "\n",
        "# Defina a imagem base que contém as bibliotecas necessárias.\n",
        "# Você pode criar uma imagem customizada ou usar uma pré-existente\n",
        "# que tenha as dependências instaladas.\n",
        "BASE_IMAGE = 'python:3.9'\n",
        "PACKAGES_TO_INSTALL = [\n",
        "    'google-cloud-bigquery[pandas]',\n",
        "    'pandas',\n",
        "    'statsmodels',\n",
        "    'scikit-learn', # Usado para joblib\n",
        "    'db-dtypes',\n",
        "    'tabulate',\n",
        "    'google-cloud-aiplatform',\n",
        "    'google-cloud-storage'\n",
        "]\n",
        "\n",
        "PROJECT_ID = \"project-poc-purple\"\n",
        "BUCKET_NAME=f\"gs://000-pipelines-root_{PROJECT_ID}\"\n",
        "\n",
        "# Create bucket\n",
        "PIPELINE_ROOT = f\"{BUCKET_NAME}/insurance_risk_pricing_pipeline/\"\n",
        "\n",
        "#=========================================================================================\n",
        "# COMPONENTE 1: Carregar Dados do BigQuery\n",
        "#=========================================================================================\n",
        "@dsl.component(\n",
        "    base_image=BASE_IMAGE,\n",
        "    packages_to_install=PACKAGES_TO_INSTALL\n",
        ")\n",
        "def load_data_from_bq(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    bq_table: str,\n",
        "    output_dataset: Output[Dataset]\n",
        "):\n",
        "    \"\"\"Carrega dados de uma tabela do BigQuery e salva como um arquivo CSV.\"\"\"\n",
        "    from google.cloud import bigquery\n",
        "    import pandas as pd\n",
        "\n",
        "    client = bigquery.Client(project=project_id, location=location)\n",
        "\n",
        "    print(f\"Lendo dados da tabela {bq_table}...\")\n",
        "    sql_query = f\"SELECT * FROM `{bq_table}`\"\n",
        "    df = client.query(sql_query).to_dataframe()\n",
        "\n",
        "    print(\"Ajuste formato colunas\")\n",
        "    colunas_para_converter = df.select_dtypes(include='Int64').columns\n",
        "    for col in colunas_para_converter:\n",
        "        df[col] = df[col].astype('int64')\n",
        "\n",
        "    print(\"✅ Sucesso! Dados carregados no DataFrame.\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Salva o dataframe como um arquivo CSV, que é o artefato de saída.\n",
        "    df.to_csv(output_dataset.path, index=False)\n",
        "    print(f\"Dataset salvo em: {output_dataset.path}\")\n",
        "\n",
        "#=========================================================================================\n",
        "# COMPONENTE 2: Treinar Modelo de Frequência (Poisson)\n",
        "#=========================================================================================\n",
        "@dsl.component(\n",
        "    base_image=BASE_IMAGE,\n",
        "    packages_to_install=PACKAGES_TO_INSTALL\n",
        ")\n",
        "def train_frequency_model(\n",
        "    input_dataset: Input[Dataset],\n",
        "    target_column: str, # 'qtd_colisao_parcial' ou 'qtd_colisao_total'\n",
        "    output_model: Output[Model]\n",
        "):\n",
        "    \"\"\"Treina um modelo GLM Poisson para frequência de sinistros.\"\"\"\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import statsmodels.api as sm\n",
        "    import joblib\n",
        "\n",
        "    df = pd.read_csv(input_dataset.path)\n",
        "\n",
        "    print(f\"Treinando modelo de frequência para a coluna: {target_column}\")\n",
        "\n",
        "    features_freq = ['classe_bonus', 'idade', 'rns']\n",
        "    X_freq = sm.add_constant(df[features_freq])\n",
        "\n",
        "    model = sm.GLM(\n",
        "        endog=df[target_column],\n",
        "        exog=X_freq,\n",
        "        family=sm.families.Poisson(),\n",
        "        offset=np.log(df['exposicao'])\n",
        "    )\n",
        "    results = model.fit()\n",
        "\n",
        "    # Salva o modelo treinado usando joblib\n",
        "    joblib.dump(results, output_model.path)\n",
        "    print(f\"Modelo de frequência salvo em: {output_model.path}\")\n",
        "    print(results.summary())\n",
        "\n",
        "#=========================================================================================\n",
        "# COMPONENTE 3: Treinar Modelo de Severidade (Gamma)\n",
        "#=========================================================================================\n",
        "@dsl.component(\n",
        "    base_image=BASE_IMAGE,\n",
        "    packages_to_install=PACKAGES_TO_INSTALL\n",
        ")\n",
        "def train_severity_model(\n",
        "    input_dataset: Input[Dataset],\n",
        "    output_model: Output[Model]\n",
        "):\n",
        "    \"\"\"Treina um modelo GLM Gamma para severidade de sinistros parciais.\"\"\"\n",
        "    import pandas as pd\n",
        "    import statsmodels.api as sm\n",
        "    import joblib\n",
        "\n",
        "    df = pd.read_csv(input_dataset.path)\n",
        "\n",
        "    print(\"Treinando modelo de severidade...\")\n",
        "\n",
        "    df_com_sinistro = df[df['valor_colisao_parcial'] > 0].copy()\n",
        "    y_sev = df_com_sinistro['valor_colisao_parcial'] / df_com_sinistro['qtd_colisao_parcial']\n",
        "\n",
        "    features_sev = ['classe_bonus', 'idade', 'rns', 'valor_veiculo']\n",
        "    X_sev_com_sinistro = sm.add_constant(df_com_sinistro[features_sev])\n",
        "\n",
        "    model = sm.GLM(\n",
        "        endog=y_sev,\n",
        "        exog=X_sev_com_sinistro,\n",
        "        family=sm.families.Gamma(link=sm.families.links.Log())\n",
        "    )\n",
        "    results = model.fit()\n",
        "    print(results.summary())\n",
        "\n",
        "    joblib.dump(results, output_model.path)\n",
        "    print(f\"Modelo de severidade salvo em: {output_model.path}\")\n",
        "    print(results.summary())\n",
        "\n",
        "#=========================================================================================\n",
        "# COMPONENTE 4: Predizer e Calcular Prêmio\n",
        "#=========================================================================================\n",
        "@dsl.component(\n",
        "    base_image=BASE_IMAGE,\n",
        "    packages_to_install=PACKAGES_TO_INSTALL\n",
        ")\n",
        "def predict_from_registered_models(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    input_dataset: Input[Dataset],\n",
        "    model_freq_parcial_resource_name: str,\n",
        "    model_freq_total_resource_name: str,\n",
        "    model_sev_parcial_resource_name: str,\n",
        "    output_predictions: Output[Dataset]\n",
        "):\n",
        "    \"\"\"Carrega modelos registrados, aplica predições e calcula o prêmio.\"\"\"\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import joblib\n",
        "    import statsmodels.api as sm\n",
        "    from google.cloud import aiplatform, storage\n",
        "    import os\n",
        "\n",
        "    df = pd.read_csv(input_dataset.path)\n",
        "    aiplatform.init(project=project_id, location=location)\n",
        "\n",
        "    def download_and_load_model(model_resource_name: str) -> any:\n",
        "        \"\"\"Baixa um modelo via Model Registry e o carrega.\"\"\"\n",
        "        print(f\"Carregando modelo do resource name: {model_resource_name}\")\n",
        "        model = aiplatform.Model(model_name=model_resource_name)\n",
        "        model_artifact_dir = model.uri\n",
        "        print(f\"URI do artefato do modelo: {model_artifact_dir}\")\n",
        "\n",
        "        model_file_gcs_path = f\"{model_artifact_dir}/model.joblib\"\n",
        "        local_model_file = f\"/tmp/{model_resource_name.split('/')[-1]}.joblib\"\n",
        "\n",
        "        path_parts = model_file_gcs_path.replace(\"gs://\", \"\").split('/')\n",
        "        bucket_name = path_parts[0]\n",
        "        object_name = '/'.join(path_parts[1:])\n",
        "\n",
        "        storage_client = storage.Client(project=project_id)\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        blob = bucket.blob(object_name)\n",
        "        blob.download_to_filename(local_model_file)\n",
        "        print(f\"Modelo baixado para {local_model_file}\")\n",
        "\n",
        "        return joblib.load(local_model_file)\n",
        "\n",
        "    # Carrega os modelos a partir do Registry\n",
        "    results_freq_parcial = download_and_load_model(model_freq_parcial_resource_name)\n",
        "    results_freq_total = download_and_load_model(model_freq_total_resource_name)\n",
        "    results_sev_parcial = download_and_load_model(model_sev_parcial_resource_name)\n",
        "    print(\"Modelos carregados com sucesso do Registry.\")\n",
        "\n",
        "    # Predição da Frequência (versão corrigida, sem o np.exp extra)\n",
        "    df['freq_parcial_pred'] = results_freq_parcial.predict(sm.add_constant(df[['classe_bonus', 'idade', 'rns']]))\n",
        "    df['freq_total_pred'] = results_freq_total.predict(sm.add_constant(df[['classe_bonus', 'idade', 'rns']]))\n",
        "\n",
        "    # Predição da Severidade (versão corrigida)\n",
        "    df['sev_parcial_pred'] = results_sev_parcial.predict(sm.add_constant(df[['classe_bonus', 'idade', 'rns', 'valor_veiculo']]))\n",
        "\n",
        "    # Cálculo do Prêmio de Risco\n",
        "    df['premio_risco_pred'] = (\n",
        "        df['freq_parcial_pred'] * df['sev_parcial_pred'] +\n",
        "        df['freq_total_pred'] * df['valor_veiculo']\n",
        "    )\n",
        "\n",
        "    print(\"Cálculos finalizados. Salvando predições.\")\n",
        "    df.to_csv(output_predictions.path, index=False)\n",
        "    print(df[['valor_veiculo', 'freq_parcial_pred', 'sev_parcial_pred', 'freq_total_pred', 'premio_risco_pred']].head())\n",
        "\n",
        "\n",
        "#=========================================================================================\n",
        "# COMPONENTE 5: Gerar Relatório de Sumarização\n",
        "#=========================================================================================\n",
        "@dsl.component(\n",
        "    base_image=BASE_IMAGE,\n",
        "    packages_to_install=PACKAGES_TO_INSTALL\n",
        ")\n",
        "def generate_summary_report(\n",
        "    predictions_dataset: Input[Dataset],\n",
        "    output_report: Output[Markdown]\n",
        "):\n",
        "    \"\"\"Calcula métricas sumarizadas e gera um relatório de comparação.\"\"\"\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    df = pd.read_csv(predictions_dataset.path)\n",
        "\n",
        "    # Métricas Realizadas\n",
        "    soma_exposicao = df['exposicao'].sum()\n",
        "    soma_qtd_parcial = df['qtd_colisao_parcial'].sum()\n",
        "    soma_valor_parcial = df['valor_colisao_parcial'].sum()\n",
        "    soma_qtd_total = df['qtd_colisao_total'].sum()\n",
        "\n",
        "    freq_parcial_real = soma_qtd_parcial / soma_exposicao\n",
        "    sev_parcial_real = soma_valor_parcial / soma_qtd_parcial if soma_qtd_parcial > 0 else 0\n",
        "    freq_total_real = soma_qtd_total / soma_exposicao\n",
        "    premio_risco_real = freq_parcial_real * sev_parcial_real + freq_total_real * np.average(df['valor_veiculo'], weights=df['exposicao'])\n",
        "\n",
        "    # Métricas Previstas\n",
        "    premio_risco_prev_total = (df['premio_risco_pred'] * df['exposicao']).sum()\n",
        "    premio_risco_prev_medio = premio_risco_prev_total / soma_exposicao\n",
        "\n",
        "    soma_qtd_parcial_pred = (df['freq_parcial_pred'] * df['exposicao']).sum()\n",
        "    soma_valor_parcial_pred = (df['freq_parcial_pred'] * df['exposicao'] * df['sev_parcial_pred']).sum()\n",
        "    soma_qtd_total_pred = (df['freq_total_pred'] * df['exposicao']).sum()\n",
        "\n",
        "    freq_parcial_prev = soma_qtd_parcial_pred / soma_exposicao\n",
        "    sev_parcial_prev = soma_valor_parcial_pred / soma_qtd_parcial_pred if soma_qtd_parcial_pred > 0 else 0\n",
        "    freq_total_prev = soma_qtd_total_pred / soma_exposicao\n",
        "    premio_risco_prev = freq_parcial_prev * sev_parcial_prev + freq_total_prev * np.average(df['valor_veiculo'], weights=df['exposicao'])\n",
        "\n",
        "    sumario = pd.DataFrame({\n",
        "        'Métrica': [\n",
        "            'Frequência de Colisão Parcial',\n",
        "            'Severidade de Colisão Parcial',\n",
        "            'Frequência de Colisão Total',\n",
        "            'Prêmio de Risco (Puro Médio)'\n",
        "        ],\n",
        "        'Valor Realizado': [\n",
        "            freq_parcial_real,\n",
        "            sev_parcial_real,\n",
        "            freq_total_real,\n",
        "            premio_risco_real\n",
        "        ],\n",
        "        'Valor Previsto pelo Modelo': [\n",
        "            freq_parcial_prev,\n",
        "            sev_parcial_prev,\n",
        "            freq_total_prev,\n",
        "            premio_risco_prev\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    # Gerando o relatório em Markdown\n",
        "    markdown_report = \"# Relatório de Comparação Real x Previsto\\n\\n\"\n",
        "    markdown_report += sumario.to_markdown(index=False)\n",
        "\n",
        "    with open(output_report.path, 'w') as f:\n",
        "        f.write(markdown_report)\n",
        "\n",
        "    print(\"Relatório gerado com sucesso!\")\n",
        "    print(\"RELATÓRIO DE COMPARAÇÃO REAL X PREVISTO\")\n",
        "    print(\"=\"*75)\n",
        "    print(sumario.to_string(index=False))\n",
        "    print(\"=\"*75)\n",
        "\n",
        "#=========================================================================================\n",
        "# COMPONENTE 6: Registrar Modelo no Vertex AI\n",
        "@dsl.component(\n",
        "    base_image=BASE_IMAGE,\n",
        "    packages_to_install=PACKAGES_TO_INSTALL\n",
        ")\n",
        "def upload_and_register_model(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    model_display_name: str,\n",
        "    model_artifact: Input[Model],\n",
        ") -> str:  # MUDANÇA 1: Declara o retorno de uma string\n",
        "    \"\"\"Faz o upload de um artefato de modelo e retorna seu resource name.\"\"\"\n",
        "    from google.cloud import aiplatform, storage\n",
        "    import os\n",
        "\n",
        "    SERVING_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-1:latest\"\n",
        "    aiplatform.init(project=project_id, location=location)\n",
        "\n",
        "    gcs_path = model_artifact.uri\n",
        "    path_parts = gcs_path.replace(\"gs://\", \"\").split('/')\n",
        "    bucket_name = path_parts[0]\n",
        "    object_prefix = '/'.join(path_parts[1:])\n",
        "    model_upload_dir = os.path.dirname(object_prefix)\n",
        "    new_file_name = \"model.joblib\"\n",
        "    new_object_path = f\"{model_upload_dir}/{new_file_name}\"\n",
        "\n",
        "    storage_client = storage.Client(project=project_id)\n",
        "    source_bucket = storage_client.bucket(bucket_name)\n",
        "    source_blob = source_bucket.blob(object_prefix)\n",
        "    source_bucket.copy_blob(source_blob, source_bucket, new_object_path)\n",
        "    print(f\"Artefato do modelo copiado para: gs://{bucket_name}/{new_object_path}\")\n",
        "\n",
        "    artifact_uri_for_upload = f\"gs://{bucket_name}/{model_upload_dir}\"\n",
        "    print(f\"Registrando o modelo '{model_display_name}' do diretório {artifact_uri_for_upload}...\")\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_display_name,\n",
        "        artifact_uri=artifact_uri_for_upload,\n",
        "        serving_container_image_uri=SERVING_IMAGE,\n",
        "        description=f\"Modelo de precificação de risco: {model_display_name}\",\n",
        "    )\n",
        "    model.wait()\n",
        "\n",
        "    resource_name = model.resource_name\n",
        "    print(f\"✅ Modelo registrado! Resource Name: {resource_name}\")\n",
        "\n",
        "    # MUDANÇA 2: Retorna a string diretamente\n",
        "    return resource_name\n",
        "\n",
        "\n",
        "#=========================================================================================\n",
        "# DEFINIÇÃO DA PIPELINE ATUALIZADAV2\n",
        "#=========================================================================================\n",
        "@dsl.pipeline(\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        "    name='Pipeline de Precificação de Risco com Registro de Modelo',\n",
        "    description='Pipeline que treina, registra e usa modelos para calcular o prêmio de risco.'\n",
        ")\n",
        "def risk_pricing_pipeline2(\n",
        "    project_id: str = 'project-poc-purple',\n",
        "    location: str = 'us-central1',  # Região para o Vertex AI (BQ pode estar em outra)\n",
        "    bq_location: str = 'US', # Localização do dataset no BigQuery\n",
        "    bq_table: str = 'project-poc-purple.demos.dados_apolices_v1',\n",
        "    model_name_prefix: str = 'risk-pricing-auto-v2'\n",
        "):\n",
        "    # Passo 1: Carregar os dados\n",
        "    load_task = load_data_from_bq(\n",
        "        project_id=project_id,\n",
        "        location=bq_location,\n",
        "        bq_table=bq_table\n",
        "    )\n",
        "     # Definição de recursos a serem alocados para esta tarefa\n",
        "    load_task.set_cpu_limit(\"2\").set_memory_limit(\"8G\")\n",
        "\n",
        "    # Passo 2: Treinar os 3 modelos (em paralelo)\n",
        "    train_freq_parcial_task = train_frequency_model(\n",
        "        input_dataset=load_task.outputs['output_dataset'],\n",
        "        target_column='qtd_colisao_parcial'\n",
        "    )\n",
        "    # Definição de recursos a serem alocados para esta tarefa\n",
        "    train_freq_parcial_task.set_cpu_limit(\"2\").set_memory_limit(\"8G\")\n",
        "\n",
        "    train_freq_total_task = train_frequency_model(\n",
        "        input_dataset=load_task.outputs['output_dataset'],\n",
        "        target_column='qtd_colisao_total'\n",
        "    )\n",
        "    # Definição de recursos a serem alocados para esta tarefa\n",
        "    train_freq_total_task.set_cpu_limit(\"2\").set_memory_limit(\"8G\")\n",
        "\n",
        "    train_sev_parcial_task = train_severity_model(\n",
        "        input_dataset=load_task.outputs['output_dataset']\n",
        "    )\n",
        "    # Definição de recursos a serem alocados para esta tarefa\n",
        "    train_sev_parcial_task.set_cpu_limit(\"2\").set_memory_limit(\"8G\")\n",
        "\n",
        "    # Passo 3: Registrar os 3 modelos no Vertex AI (em paralelo)\n",
        "    register_freq_parcial_task = upload_and_register_model(\n",
        "        project_id=project_id,\n",
        "        location=location,\n",
        "        model_display_name=f\"{model_name_prefix}-freq-parcial\",\n",
        "        model_artifact=train_freq_parcial_task.outputs['output_model']\n",
        "    )\n",
        "    # Definição de recursos a serem alocados para esta tarefa\n",
        "    register_freq_parcial_task.set_cpu_limit(\"2\").set_memory_limit(\"8G\")\n",
        "\n",
        "    register_freq_total_task = upload_and_register_model(\n",
        "        project_id=project_id,\n",
        "        location=location,\n",
        "        model_display_name=f\"{model_name_prefix}-freq-total\",\n",
        "        model_artifact=train_freq_total_task.outputs['output_model']\n",
        "    )\n",
        "    # Definição de recursos a serem alocados para esta tarefa\n",
        "    register_freq_total_task.set_cpu_limit(\"2\").set_memory_limit(\"8G\")\n",
        "\n",
        "    register_sev_parcial_task = upload_and_register_model(\n",
        "        project_id=project_id,\n",
        "        location=location,\n",
        "        model_display_name=f\"{model_name_prefix}-sev-parcial\",\n",
        "        model_artifact=train_sev_parcial_task.outputs['output_model']\n",
        "    )\n",
        "    # Definição de recursos a serem alocados para esta tarefa\n",
        "    register_sev_parcial_task.set_cpu_limit(\"2\").set_memory_limit(\"8G\")\n",
        "\n",
        "    # Passo 4: Fazer predições usando os 3 modelos REGISTRADOS\n",
        "    predict_task = predict_from_registered_models(\n",
        "        project_id=project_id,\n",
        "        location=location,\n",
        "        input_dataset=load_task.outputs['output_dataset'],\n",
        "        model_freq_parcial_resource_name=register_freq_parcial_task.output,\n",
        "        model_freq_total_resource_name=register_freq_total_task.output,\n",
        "        model_sev_parcial_resource_name=register_sev_parcial_task.output\n",
        "    )\n",
        "    # Definição de recursos a serem alocados para esta tarefa\n",
        "    predict_task.set_cpu_limit(\"2\").set_memory_limit(\"8G\")\n",
        "\n",
        "\n",
        "    # Passo 5: Gerar relatório final\n",
        "    generate_summary_report(\n",
        "        predictions_dataset=predict_task.outputs['output_predictions']\n",
        "    )\n",
        "    # Definição de recursos a serem alocados para esta tarefa\n",
        "    generate_summary_report.set_cpu_limit(\"2\").set_memory_limit(\"8G\")"
      ],
      "metadata": {
        "id": "ne6kfBGKYrbp"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#=========================================================================================\n",
        "# COMPILAÇÃO DA PIPELINE\n",
        "#=========================================================================================\n",
        "from kfp.compiler import Compiler\n",
        "Compiler().compile(\n",
        "    pipeline_func=risk_pricing_pipeline2,\n",
        "    package_path='risk_pricing_pipeline.yaml' # ou .json\n",
        ")\n",
        "print(\"Pipeline compilada com sucesso para 'risk_pricing_pipeline.yaml'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjBKYHp1bdVz",
        "outputId": "b973c36a-f5d4-4dd4-ee51-68029c95c52a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline compilada com sucesso para 'risk_pricing_pipeline.yaml'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#=========================================================================================\n",
        "# EXECUÇÃO DA PIPELINE\n",
        "#=========================================================================================\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud.aiplatform import pipeline_jobs\n",
        "\n",
        "PROJECT_ID = \"project-poc-purple\"\n",
        "REGION = \"us-central1\"\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "start_pipeline = pipeline_jobs.PipelineJob(\n",
        "  display_name=\"risk_pricing_pipeline-pipeline\",\n",
        "  template_path=\"risk_pricing_pipeline.yaml\",\n",
        "  enable_caching=False,\n",
        "  location=REGION\n",
        ")\n",
        "\n",
        "start_pipeline.run()"
      ],
      "metadata": {
        "id": "Gs1cMQDNcOlQ"
      },
      "execution_count": 38,
      "outputs": []
    }
  ]
}