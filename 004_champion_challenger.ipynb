{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "004-chmpion-challenger.ipynb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install kfp google-cloud-pipeline-components --upgrade -q\n",
        "!pip install google-cloud-aiplatform --upgrade -q\n",
        "!pip install google-cloud-storage --upgrade -q"
      ],
      "metadata": {
        "id": "dNp-AEI6Y010"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kfp\n",
        "from kfp import dsl\n",
        "from kfp.dsl import Input, Output, Dataset, Model, Markdown\n",
        "\n",
        "# Defina a imagem base que contém as bibliotecas necessárias.\n",
        "BASE_IMAGE = 'python:3.9'\n",
        "PACKAGES_TO_INSTALL = [\n",
        "    'google-cloud-bigquery[pandas]',\n",
        "    'pandas',\n",
        "    'statsmodels',\n",
        "    'scikit-learn',\n",
        "    'db-dtypes',\n",
        "    'tabulate',\n",
        "    'google-cloud-aiplatform',\n",
        "    'google-cloud-storage'\n",
        "]\n",
        "\n",
        "PROJECT_ID = \"project-poc-purple\"\n",
        "BUCKET_NAME=f\"gs://000-pipelines-root_{PROJECT_ID}\"\n",
        "\n",
        "# Create bucket\n",
        "PIPELINE_ROOT = f\"{BUCKET_NAME}/insurance_risk_pricing_pipeline/\"\n",
        "\n",
        "#=========================================================================================\n",
        "# COMPONENTE 1: Carregar Dados do BigQuery\n",
        "#=========================================================================================\n",
        "@dsl.component(\n",
        "    base_image=BASE_IMAGE,\n",
        "    packages_to_install=PACKAGES_TO_INSTALL\n",
        ")\n",
        "def load_data_from_bq(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    bq_table: str,\n",
        "    output_dataset: Output[Dataset]\n",
        "):\n",
        "    \"\"\"Carrega dados de uma tabela do BigQuery e salva como um arquivo CSV.\"\"\"\n",
        "    from google.cloud import bigquery\n",
        "    import pandas as pd\n",
        "\n",
        "    client = bigquery.Client(project=project_id, location=location)\n",
        "\n",
        "    print(f\"Lendo dados da tabela {bq_table}...\")\n",
        "    sql_query = f\"SELECT * FROM `{bq_table}`\"\n",
        "    df = client.query(sql_query).to_dataframe()\n",
        "\n",
        "    print(\"Ajuste formato colunas\")\n",
        "    colunas_para_converter = df.select_dtypes(include='Int64').columns\n",
        "    for col in colunas_para_converter:\n",
        "        df[col] = df[col].astype('int64')\n",
        "\n",
        "    print(\"✅ Sucesso! Dados carregados no DataFrame.\")\n",
        "    print(df.head())\n",
        "\n",
        "    df.to_csv(output_dataset.path, index=False)\n",
        "    print(f\"Dataset salvo em: {output_dataset.path}\")\n",
        "\n",
        "#=========================================================================================\n",
        "# COMPONENTE 2: Treinar Modelo de Frequência (Poisson)\n",
        "#=========================================================================================\n",
        "@dsl.component(\n",
        "    base_image=BASE_IMAGE,\n",
        "    packages_to_install=PACKAGES_TO_INSTALL\n",
        ")\n",
        "def train_frequency_model(\n",
        "    input_dataset: Input[Dataset],\n",
        "    target_column: str, # 'qtd_colisao_parcial' ou 'qtd_colisao_total'\n",
        "    output_model: Output[Model]\n",
        "):\n",
        "    \"\"\"Treina um modelo GLM Poisson para frequência de sinistros.\"\"\"\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import statsmodels.api as sm\n",
        "    import joblib\n",
        "\n",
        "    df = pd.read_csv(input_dataset.path)\n",
        "\n",
        "    print(f\"Treinando modelo de frequência para a coluna: {target_column}\")\n",
        "\n",
        "    features_freq = ['classe_bonus', 'idade', 'rns']\n",
        "    X_freq = sm.add_constant(df[features_freq])\n",
        "\n",
        "    model = sm.GLM(\n",
        "        endog=df[target_column],\n",
        "        exog=X_freq,\n",
        "        family=sm.families.Poisson(),\n",
        "        offset=np.log(df['exposicao'])\n",
        "    )\n",
        "    results = model.fit()\n",
        "\n",
        "    joblib.dump(results, output_model.path)\n",
        "    print(f\"Modelo de frequência salvo em: {output_model.path}\")\n",
        "    print(results.summary())\n",
        "\n",
        "#=========================================================================================\n",
        "# COMPONENTE 3: Treinar Modelo de Severidade (Gamma)\n",
        "#=========================================================================================\n",
        "@dsl.component(\n",
        "    base_image=BASE_IMAGE,\n",
        "    packages_to_install=PACKAGES_TO_INSTALL\n",
        ")\n",
        "def train_severity_model(\n",
        "    input_dataset: Input[Dataset],\n",
        "    output_model: Output[Model]\n",
        "):\n",
        "    \"\"\"Treina um modelo GLM Gamma para severidade de sinistros parciais.\"\"\"\n",
        "    import pandas as pd\n",
        "    import statsmodels.api as sm\n",
        "    import joblib\n",
        "\n",
        "    df = pd.read_csv(input_dataset.path)\n",
        "\n",
        "    print(\"Treinando modelo de severidade...\")\n",
        "\n",
        "    df_com_sinistro = df[df['valor_colisao_parcial'] > 0].copy()\n",
        "    y_sev = df_com_sinistro['valor_colisao_parcial'] / df_com_sinistro['qtd_colisao_parcial']\n",
        "\n",
        "    features_sev = ['classe_bonus', 'idade', 'rns', 'valor_veiculo']\n",
        "    X_sev_com_sinistro = sm.add_constant(df_com_sinistro[features_sev])\n",
        "\n",
        "    model = sm.GLM(\n",
        "        endog=y_sev,\n",
        "        exog=X_sev_com_sinistro,\n",
        "        family=sm.families.Gamma(link=sm.families.links.Log())\n",
        "    )\n",
        "    results = model.fit()\n",
        "\n",
        "    joblib.dump(results, output_model.path)\n",
        "    print(f\"Modelo de severidade salvo em: {output_model.path}\")\n",
        "    print(results.summary())\n",
        "\n",
        "#=========================================================================================\n",
        "# NOVO COMPONENTE 4: Avaliar e Registrar Modelo com Lógica Champion-Challenger\n",
        "#=========================================================================================\n",
        "@dsl.component(\n",
        "    base_image=BASE_IMAGE,\n",
        "    packages_to_install=PACKAGES_TO_INSTALL,\n",
        ")\n",
        "def evaluate_and_register_challenger(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    model_display_name: str,\n",
        "    model_type: str, # 'frequency' or 'severity'\n",
        "    target_column: str, # Nome da coluna alvo para o cálculo da métrica\n",
        "    challenger_model: Input[Model],\n",
        "    evaluation_dataset: Input[Dataset],\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Usa um método de busca explícito e corrige o download do artefato do campeão.\n",
        "    \"\"\"\n",
        "    import traceback\n",
        "    try:\n",
        "        import pandas as pd\n",
        "        import numpy as np\n",
        "        import statsmodels.api as sm\n",
        "        import joblib\n",
        "        from google.cloud import aiplatform, storage\n",
        "        from google.api_core import exceptions\n",
        "        import os\n",
        "\n",
        "        print(f\"--- INICIANDO AVALIAÇÃO PARA O MODELO: {model_display_name} ---\")\n",
        "\n",
        "        aiplatform.init(project=project_id, location=location)\n",
        "        df = pd.read_csv(evaluation_dataset.path)\n",
        "\n",
        "        def get_model_score(model_path: str) -> float:\n",
        "            model = joblib.load(model_path)\n",
        "            if model_type == 'frequency':\n",
        "                if df['exposicao'].sum() == 0: return float('inf')\n",
        "                features = ['classe_bonus', 'idade', 'rns']\n",
        "                X = sm.add_constant(df[features])\n",
        "                df['pred'] = model.predict(X)\n",
        "                real_freq = df[target_column].sum() / df['exposicao'].sum()\n",
        "                pred_freq = (df['pred'] * df['exposicao']).sum() / df['exposicao'].sum()\n",
        "                return abs(real_freq - pred_freq)\n",
        "            elif model_type == 'severity':\n",
        "                df_claimed = df[df[target_column] > 0].copy()\n",
        "                if df_claimed.empty: return float('inf')\n",
        "                df_claimed = df_claimed[df_claimed['qtd_colisao_parcial'] > 0]\n",
        "                if df_claimed.empty: return float('inf')\n",
        "                real_sev_per_claim = df_claimed[target_column] / df_claimed['qtd_colisao_parcial']\n",
        "                features = ['classe_bonus', 'idade', 'rns', 'valor_veiculo']\n",
        "                X = sm.add_constant(df_claimed[features])\n",
        "                pred_sev_per_claim = model.predict(X)\n",
        "                return abs(real_sev_per_claim.mean() - pred_sev_per_claim.mean())\n",
        "            return float('inf')\n",
        "\n",
        "        challenger_score = get_model_score(challenger_model.path)\n",
        "        print(f\"Score do Challenger ({model_display_name}): {challenger_score}\")\n",
        "\n",
        "        champion_model = None\n",
        "        champion_score = float('inf')\n",
        "        champion_model_resource_name = \"\"\n",
        "        model_list = []\n",
        "        try:\n",
        "            print(f\"Buscando modelos com display name: '{model_display_name}'\")\n",
        "            model_list = aiplatform.Model.list(filter=f'display_name=\"{model_display_name}\"')\n",
        "\n",
        "            found_champion = False\n",
        "            for model_version in model_list:\n",
        "                if \"default\" in model_version.version_aliases:\n",
        "                    champion_model = model_version\n",
        "                    champion_model_resource_name = champion_model.resource_name\n",
        "                    print(f\"Versão default (campeã) encontrada: {champion_model_resource_name}\")\n",
        "                    found_champion = True\n",
        "                    break\n",
        "\n",
        "            if not found_champion:\n",
        "                raise exceptions.NotFound(f\"Nenhuma versão com o alias 'default' foi encontrada para o display name '{model_display_name}'.\")\n",
        "\n",
        "            local_champion_path = \"/tmp/champion_model.joblib\"\n",
        "            storage_client = storage.Client(project=project_id)\n",
        "\n",
        "            # --- CORREÇÃO FINAL: Manter o prefixo 'gs://' no URI ---\n",
        "            model_uri = champion_model.uri\n",
        "            if not model_uri.endswith('/'):\n",
        "                model_uri += '/'\n",
        "            blob = storage.Blob.from_string(model_uri + \"model.joblib\", client=storage_client)\n",
        "            # --- FIM DA CORREÇÃO ---\n",
        "\n",
        "            blob.download_to_filename(local_champion_path)\n",
        "            print(\"Artefato do modelo campeão baixado com sucesso.\")\n",
        "            champion_score = get_model_score(local_champion_path)\n",
        "            print(f\"Score da versão default ({model_display_name}): {champion_score}\")\n",
        "\n",
        "        except exceptions.NotFound as e:\n",
        "            print(f\"{e} Challenger vence por padrão.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Um erro ocorreu ao buscar/baixar a versão default. O challenger será considerado vencedor. Erro: {e}\")\n",
        "\n",
        "        if challenger_score < champion_score:\n",
        "            print(\"Challenger é melhor. Promovendo para nova versão default.\")\n",
        "            gcs_uri = challenger_model.uri\n",
        "            path_parts = gcs_uri.replace(\"gs://\", \"\").split('/')\n",
        "            bucket_name = path_parts[0]\n",
        "            object_name = '/'.join(path_parts[1:])\n",
        "            object_dir = os.path.dirname(object_name)\n",
        "            artifact_uri_for_upload = f\"gs://{bucket_name}/{object_dir}\"\n",
        "            new_object_name = f\"{object_dir}/model.joblib\"\n",
        "            storage_client = storage.Client(project=project_id)\n",
        "            source_bucket = storage_client.bucket(bucket_name)\n",
        "            source_blob = source_bucket.blob(object_name)\n",
        "            source_bucket.copy_blob(source_blob, source_bucket, new_object_name)\n",
        "\n",
        "            parent_model_ref = None\n",
        "            if model_list:\n",
        "                parent_model_ref = model_list[0].resource_name.split('@')[0]\n",
        "                print(f\"Modelo pai encontrado para versionamento: {parent_model_ref}\")\n",
        "\n",
        "            new_champion = aiplatform.Model.upload(\n",
        "                display_name=model_display_name,\n",
        "                artifact_uri=artifact_uri_for_upload,\n",
        "                serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-1:latest\",\n",
        "                parent_model=parent_model_ref,\n",
        "                is_default_version=True\n",
        "            )\n",
        "            new_champion.wait()\n",
        "            print(f\"Upload bem-sucedido. Nova versão default: {new_champion.resource_name} (Versão: {new_champion.version_id})\")\n",
        "\n",
        "            print(f\"--- AVALIAÇÃO CONCLUÍDA. NOVO DEFAULT: {new_champion.resource_name} ---\")\n",
        "            return new_champion.resource_name\n",
        "        else:\n",
        "            print(\"Versão default atual é melhor ou igual. Mantendo a versão existente.\")\n",
        "            print(f\"--- AVALIAÇÃO CONCLUÍDA. DEFAULT MANTIDO: {champion_model_resource_name} ---\")\n",
        "            return champion_model_resource_name\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"!!!!!!!!!! ERRO FATAL NO COMPONENTE !!!!!!!!!!\")\n",
        "        traceback.print_exc()\n",
        "        raise e\n",
        "\n",
        "#=========================================================================================\n",
        "# COMPONENTE 5: Predizer e Calcular Prêmio\n",
        "#=========================================================================================\n",
        "@dsl.component(\n",
        "    base_image=BASE_IMAGE,\n",
        "    packages_to_install=PACKAGES_TO_INSTALL\n",
        ")\n",
        "def predict_from_registered_models(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    input_dataset: Input[Dataset],\n",
        "    model_freq_parcial_resource_name: str,\n",
        "    model_freq_total_resource_name: str,\n",
        "    model_sev_parcial_resource_name: str,\n",
        "    output_predictions: Output[Dataset]\n",
        "):\n",
        "    \"\"\"Carrega modelos registrados, aplica predições e calcula o prêmio.\"\"\"\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import joblib\n",
        "    import statsmodels.api as sm\n",
        "    from google.cloud import aiplatform, storage\n",
        "    import os\n",
        "\n",
        "    df = pd.read_csv(input_dataset.path)\n",
        "    aiplatform.init(project=project_id, location=location)\n",
        "\n",
        "    def download_and_load_model(model_resource_name: str) -> any:\n",
        "        \"\"\"Baixa um modelo via Model Registry e o carrega.\"\"\"\n",
        "        print(f\"Carregando modelo do resource name: {model_resource_name}\")\n",
        "        model = aiplatform.Model(model_name=model_resource_name)\n",
        "        model_artifact_dir = model.uri\n",
        "        print(f\"URI do artefato do modelo: {model_artifact_dir}\")\n",
        "\n",
        "        # Nome do arquivo de modelo padrão salvo pelo Vertex AI\n",
        "        model_file_name = 'model.joblib'\n",
        "        # Caso o upload tenha sido de uma versão mais antiga da plataforma, pode ser 'model.pkl'\n",
        "        # Esta lógica tenta encontrar o arquivo de modelo correto.\n",
        "        storage_client = storage.Client(project=project_id)\n",
        "        bucket_name = model_artifact_dir.split('/')[2]\n",
        "        prefix = '/'.join(model_artifact_dir.split('/')[3:]) + '/'\n",
        "        blobs = storage_client.list_blobs(bucket_name, prefix=prefix)\n",
        "        model_blob_name = None\n",
        "        for blob in blobs:\n",
        "            if blob.name.endswith('.joblib') or blob.name.endswith('.pkl'):\n",
        "                model_blob_name = blob.name\n",
        "                break\n",
        "\n",
        "        if not model_blob_name:\n",
        "            raise FileNotFoundError(f\"Não foi possível encontrar um arquivo .joblib ou .pkl em {model_artifact_dir}\")\n",
        "\n",
        "        local_model_file = f\"/tmp/{model_resource_name.split('/')[-1]}.joblib\"\n",
        "\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        blob = bucket.blob(model_blob_name)\n",
        "        blob.download_to_filename(local_model_file)\n",
        "        print(f\"Modelo baixado para {local_model_file}\")\n",
        "\n",
        "        return joblib.load(local_model_file)\n",
        "\n",
        "    results_freq_parcial = download_and_load_model(model_freq_parcial_resource_name)\n",
        "    results_freq_total = download_and_load_model(model_freq_total_resource_name)\n",
        "    results_sev_parcial = download_and_load_model(model_sev_parcial_resource_name)\n",
        "    print(\"Modelos carregados com sucesso do Registry.\")\n",
        "\n",
        "    df['freq_parcial_pred'] = results_freq_parcial.predict(sm.add_constant(df[['classe_bonus', 'idade', 'rns']]))\n",
        "    df['freq_total_pred'] = results_freq_total.predict(sm.add_constant(df[['classe_bonus', 'idade', 'rns']]))\n",
        "    df['sev_parcial_pred'] = results_sev_parcial.predict(sm.add_constant(df[['classe_bonus', 'idade', 'rns', 'valor_veiculo']]))\n",
        "\n",
        "    df['premio_risco_pred'] = (df['freq_parcial_pred'] * df['sev_parcial_pred'] + df['freq_total_pred'] * df['valor_veiculo'])\n",
        "\n",
        "    print(\"Cálculos finalizados. Salvando predições.\")\n",
        "    df.to_csv(output_predictions.path, index=False)\n",
        "    print(df[['valor_veiculo', 'freq_parcial_pred', 'sev_parcial_pred', 'freq_total_pred', 'premio_risco_pred']].head())\n",
        "\n",
        "#=========================================================================================\n",
        "# COMPONENTE 6: Gerar Relatório de Sumarização (sem alterações)\n",
        "#=========================================================================================\n",
        "@dsl.component(\n",
        "    base_image=BASE_IMAGE,\n",
        "    packages_to_install=PACKAGES_TO_INSTALL\n",
        ")\n",
        "def generate_summary_report(\n",
        "    predictions_dataset: Input[Dataset],\n",
        "    output_report: Output[Markdown]\n",
        "):\n",
        "    \"\"\"Calcula métricas sumarizadas e gera um relatório de comparação.\"\"\"\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    df = pd.read_csv(predictions_dataset.path)\n",
        "\n",
        "    soma_exposicao = df['exposicao'].sum()\n",
        "    soma_qtd_parcial = df['qtd_colisao_parcial'].sum()\n",
        "    soma_valor_parcial = df['valor_colisao_parcial'].sum()\n",
        "    soma_qtd_total = df['qtd_colisao_total'].sum()\n",
        "\n",
        "    freq_parcial_real = soma_qtd_parcial / soma_exposicao\n",
        "    sev_parcial_real = soma_valor_parcial / soma_qtd_parcial if soma_qtd_parcial > 0 else 0\n",
        "    freq_total_real = soma_qtd_total / soma_exposicao\n",
        "\n",
        "    premio_risco_real = freq_parcial_real * sev_parcial_real + freq_total_real * np.average(df['valor_veiculo'], weights=df['exposicao'])\n",
        "    premio_risco_prev_total = (df['premio_risco_pred'] * df['exposicao']).sum()\n",
        "    premio_risco_prev_medio = premio_risco_prev_total / soma_exposicao\n",
        "\n",
        "    soma_qtd_parcial_pred = (df['freq_parcial_pred'] * df['exposicao']).sum()\n",
        "    soma_valor_parcial_pred = (df['freq_parcial_pred'] * df['exposicao'] * df['sev_parcial_pred']).sum()\n",
        "    soma_qtd_total_pred = (df['freq_total_pred'] * df['exposicao']).sum()\n",
        "\n",
        "    freq_parcial_prev = soma_qtd_parcial_pred / soma_exposicao\n",
        "    sev_parcial_prev = soma_valor_parcial_pred / soma_qtd_parcial_pred if soma_qtd_parcial_pred > 0 else 0\n",
        "    freq_total_prev = soma_qtd_total_pred / soma_exposicao\n",
        "    premio_risco_prev = freq_parcial_prev * sev_parcial_prev + freq_total_prev * np.average(df['valor_veiculo'], weights=df['exposicao'])\n",
        "\n",
        "    sumario = pd.DataFrame({\n",
        "        'Métrica': ['Frequência de Colisão Parcial', 'Severidade de Colisão Parcial', 'Frequência de Colisão Total', 'Prêmio de Risco (Puro Médio)'],\n",
        "        'Valor Realizado': [freq_parcial_real, sev_parcial_real, freq_total_real, premio_risco_real],\n",
        "        'Valor Previsto pelo Modelo': [freq_parcial_prev, sev_parcial_prev, freq_total_prev, premio_risco_prev]\n",
        "    })\n",
        "    markdown_report = \"# Relatório de Comparação Real x Previsto\\n\\n\" + sumario.to_markdown(index=False)\n",
        "    with open(output_report.path, 'w') as f:\n",
        "        f.write(markdown_report)\n",
        "    print(\"Relatório gerado com sucesso!\")\n",
        "    print(\"RELATÓRIO DE COMPARAÇÃO REAL X PREVISTO\")\n",
        "    print(\"=\"*75)\n",
        "    print(sumario.to_string(index=False))\n",
        "    print(\"=\"*75)\n",
        "\n",
        "#=========================================================================================\n",
        "# DEFINIÇÃO DA PIPELINE ATUALIZADA COM LÓGICA CHAMPION-CHALLENGER\n",
        "#=========================================================================================\n",
        "@dsl.pipeline(\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        "    name='pipeline-precificacao-risco-champion-challenger',\n",
        "    description='Treina, avalia (champion-challenger) e registra modelos para calcular prêmio de risco.'\n",
        ")\n",
        "def risk_pricing_pipeline_champion_flow(\n",
        "    project_id: str = 'project-poc-purple',\n",
        "    location: str = 'us-central1',\n",
        "    bq_location: str = 'US',\n",
        "    bq_table: str = 'project-poc-purple.demos.dados_apolices_v1',\n",
        "    model_name_prefix: str = 'risk-pricing-ins-v3'\n",
        "):\n",
        "    load_task = load_data_from_bq(\n",
        "        project_id=project_id, location=bq_location, bq_table=bq_table\n",
        "    )\n",
        "\n",
        "    # --- Trilha Frequência Parcial ---\n",
        "    train_freq_parcial_task = train_frequency_model(\n",
        "        input_dataset=load_task.outputs['output_dataset'], target_column='qtd_colisao_parcial'\n",
        "    )\n",
        "\n",
        "    eval_freq_parcial_task = evaluate_and_register_challenger(\n",
        "        project_id=project_id, location=location,\n",
        "        model_display_name=f\"{model_name_prefix}-freq-parcial\",\n",
        "        model_type='frequency', target_column='qtd_colisao_parcial',\n",
        "        challenger_model=train_freq_parcial_task.outputs['output_model'],\n",
        "        evaluation_dataset=load_task.outputs['output_dataset']\n",
        "    )\n",
        "\n",
        "    # --- Trilha Frequência Total ---\n",
        "    train_freq_total_task = train_frequency_model(\n",
        "        input_dataset=load_task.outputs['output_dataset'], target_column='qtd_colisao_total'\n",
        "    )\n",
        "\n",
        "    eval_freq_total_task = evaluate_and_register_challenger(\n",
        "        project_id=project_id, location=location,\n",
        "        model_display_name=f\"{model_name_prefix}-freq-total\",\n",
        "        model_type='frequency', target_column='qtd_colisao_total',\n",
        "        challenger_model=train_freq_total_task.outputs['output_model'],\n",
        "        evaluation_dataset=load_task.outputs['output_dataset']\n",
        "    )\n",
        "\n",
        "    # --- Trilha Severidade Parcial ---\n",
        "    train_sev_parcial_task = train_severity_model(\n",
        "        input_dataset=load_task.outputs['output_dataset']\n",
        "    )\n",
        "\n",
        "    eval_sev_parcial_task = evaluate_and_register_challenger(\n",
        "        project_id=project_id, location=location,\n",
        "        model_display_name=f\"{model_name_prefix}-sev-parcial\",\n",
        "        model_type='severity', target_column='valor_colisao_parcial',\n",
        "        challenger_model=train_sev_parcial_task.outputs['output_model'],\n",
        "        evaluation_dataset=load_task.outputs['output_dataset']\n",
        "    )\n",
        "\n",
        "    # --- Etapa de Predição usando os modelos campeões ---\n",
        "    predict_task = predict_from_registered_models(\n",
        "        project_id=project_id, location=location,\n",
        "        input_dataset=load_task.outputs['output_dataset'],\n",
        "        model_freq_parcial_resource_name=eval_freq_parcial_task.output,\n",
        "        model_freq_total_resource_name=eval_freq_total_task.output,\n",
        "        model_sev_parcial_resource_name=eval_sev_parcial_task.output\n",
        "    )\n",
        "\n",
        "    # --- Etapa Final de Geração de Relatório ---\n",
        "    report_task = generate_summary_report(\n",
        "        predictions_dataset=predict_task.outputs['output_predictions']\n",
        "    )"
      ],
      "metadata": {
        "id": "ne6kfBGKYrbp"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#=========================================================================================\n",
        "# COMPILAÇÃO DA PIPELINE\n",
        "#=========================================================================================\n",
        "from kfp.compiler import Compiler\n",
        "\n",
        "PIPELINE_FILE = 'risk_pricing_champion_challenger.yaml'\n",
        "\n",
        "Compiler().compile(\n",
        "    pipeline_func=risk_pricing_pipeline_champion_flow,\n",
        "    package_path=PIPELINE_FILE\n",
        ")\n",
        "print(f\"Pipeline compilada com sucesso para '{PIPELINE_FILE}'\")"
      ],
      "metadata": {
        "id": "VjBKYHp1bdVz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "634d54f9-6640-45b7-c578-cffe695bc68e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline compilada com sucesso para 'risk_pricing_champion_challenger.yaml'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#=========================================================================================\n",
        "# EXECUÇÃO DA PIPELINE\n",
        "#=========================================================================================\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "PROJECT_ID = \"project-poc-purple\"\n",
        "REGION = \"us-central1\"\n",
        "PIPELINE_FILE = 'risk_pricing_champion_challenger.yaml'\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "job = aiplatform.PipelineJob(\n",
        "  display_name=\"risk-pricing-champion-challenger-pipeline\",\n",
        "  template_path=PIPELINE_FILE,\n",
        "  enable_caching=True,\n",
        "  location=REGION\n",
        ")\n",
        "\n",
        "job.run()"
      ],
      "metadata": {
        "id": "Gs1cMQDNcOlQ"
      },
      "execution_count": 35,
      "outputs": []
    }
  ]
}